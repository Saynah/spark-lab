{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "from pyspark.mllib.recommendation import Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORES_PER_NODE = 2\n",
    "NUM_WORKERS = 4\n",
    "REP_FACTOR = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the ratings file (fromUserId, toUserId, rating).  These ratings are 0-9.\n",
    "ratings_raw_DF = sqlContext.read.format(\"com.databricks.spark.csv\") \\\n",
    "                           .options(header=\"false\") \\\n",
    "                           .load(\"s3n://insight-spark-after-dark/ratings.csv.gz\") \\\n",
    "                           .repartition(CORES_PER_NODE*NUM_WORKERS*REP_FACTOR)\\\n",
    "                           .persist(StorageLevel.MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register the ratings_raw DataFrame as a temp table\n",
    "ratings_raw_DF.registerTempTable(\"ratings_raw_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17359346"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_raw_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(C0=u'1', C1=u'4062', C2=u'3'),\n",
       " Row(C0=u'1', C1=u'19727', C2=u'10'),\n",
       " Row(C0=u'1', C1=u'37184', C2=u'9'),\n",
       " Row(C0=u'1', C1=u'51524', C2=u'6'),\n",
       " Row(C0=u'1', C1=u'71042', C2=u'5')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_raw_DF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cast the DataFrame to enforce a schema with (from_user_id, to_user_id, rating)\n",
    "ratings_DF = sqlContext.sql(\"\"\"\n",
    "SELECT\n",
    "    CAST(C0 as int) AS from_user_id,\n",
    "    CAST(C1 as int) AS to_user_id,\n",
    "    CAST(C2 as int) AS rating\n",
    "FROM \n",
    "    ratings_raw_tbl\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=3751, rating=7.0),\n",
       " Rating(user=1, product=19231, rating=5.0),\n",
       " Rating(user=1, product=36750, rating=2.0),\n",
       " Rating(user=1, product=51399, rating=7.0),\n",
       " Rating(user=1, product=70694, rating=8.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create mllib.recommendation.Rating RDD from ratings DataFrame\n",
    "ratings_RDD = ratings_DF.rdd.map(lambda r: Rating(r.from_user_id, r.to_user_id, r.rating))\n",
    "\n",
    "ratings_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate ratings data into training data (80%) and test data (20%)\n",
    "split_ratings_RDD = ratings_RDD.randomSplit([0.8, 0.2])\n",
    "training_ratings_RDD = split_ratings_RDD[0]\n",
    "test_ratings_RDD = split_ratings_RDD[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the ALS model using the training data and various model hyperparameters\n",
    "model = ALS.train(training_ratings_RDD, 1, 5, 0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert known test data to have only (from, to)\n",
    "test_from_to_RDD = test_ratings_RDD.map(lambda r: (r[0], r[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=116685, product=193370, rating=3.9002635351928348),\n",
       " Rating(user=54499, product=193370, rating=2.984109066463475),\n",
       " Rating(user=58863, product=108150, rating=10.032405957363949),\n",
       " Rating(user=57163, product=108150, rating=9.413207908263985),\n",
       " Rating(user=107380, product=108150, rating=9.182127511317617)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model by predicting the ratings for the known test data\n",
    "actual_predictions_RDD = model.predictAll(test_from_to_RDD)\n",
    "\n",
    "actual_predictions_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare the known test predictions and actual predictions for comparison keyed by (from, to)\n",
    "actual_predictions_RDD = actual_predictions_RDD.map(lambda r: ((r[0], r[1]), r[2]))\n",
    "test_predictions_RDD = test_ratings_RDD.map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((10000, 117276), (5, 4.270460474679936)),\n",
       " ((60027, 168951), (4, 4.634894943158201)),\n",
       " ((121989, 99783), (1, 2.0501199552357434)),\n",
       " ((113819, 86977), (10, 7.762318729967319)),\n",
       " ((133732, 49038), (10, 8.392107996765276)),\n",
       " ((131976, 175554), (8, 8.440695135691385)),\n",
       " ((11556, 64056), (9, 9.293104485887511)),\n",
       " ((48324, 37330), (6, 8.723768284233415)),\n",
       " ((8305, 101387), (10, 10.20807837129098)),\n",
       " ((33662, 45352), (5, 5.333782872208303))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the known test predictions with the actual predictions\n",
    "test_to_actual_ratings_RDD = test_predictions_RDD.join(actual_predictions_RDD)\n",
    "test_to_actual_ratings_RDD.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.91554240348\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using Mean Absolute Error (MAE) between the known test ratings and the actual predictions \n",
    "mean_absolute_rating_error = test_to_actual_ratings_RDD.map(lambda r: abs(r[1][0]-r[1][1]))\\\n",
    "                                                       .mean()\n",
    "\n",
    "print mean_absolute_rating_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
