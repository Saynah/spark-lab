{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Spark will usually split your data into a set of partitions automatically but there are cases where you want to do this manually to improve performance. You can us the repartition() method to define the number of partitions in your RDD. In this tutorial we explore the number of partitions on performance.\n",
    "\n",
    "We are going to be using the Reddit comments dataset for this tutorial. More information on this dataset can be found [here](https://sites.google.com/a/insightdatascience.com/spark-lab/s3-data/reddit-comments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = [StructField(\"archived\", BooleanType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"body\", StringType(), True),\n",
    "        StructField(\"controversiality\", LongType(), True),\n",
    "        StructField(\"created_utc\", StringType(), True),\n",
    "        StructField(\"day\", LongType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"downs\", LongType(), True),\n",
    "        StructField(\"edited\", StringType(), True),\n",
    "        StructField(\"gilded\", LongType(), True),\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"link_id\", StringType(), True),\n",
    "        StructField(\"month\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"parent_id\", StringType(), True),\n",
    "        StructField(\"retrieved_on\", LongType(), True),\n",
    "        StructField(\"score\", LongType(), True),\n",
    "        StructField(\"score_hidden\", BooleanType(), True),\n",
    "        StructField(\"subreddit\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), True),\n",
    "        StructField(\"ups\", LongType(), True),\n",
    "        StructField(\"year\", LongType(), True)]\n",
    "rawDF = sqlContext.read.json(\"s3n://reddit-comments/2008\", StructType(fields))\n",
    "rawDF.persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "rawDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism - example 1\n",
    "\n",
    "To analyze the performance of a simple Spark job with different numbers of partitions, let's split our data into varying numbers of partitions. We'll persist the partitions to save recomputing down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting up DataFrames to have various number of partitions ranging from 2 to 32\n",
    "# All will be persisted in memory and the same job will be run on each one to show \n",
    "# performance benefits of different number of partitions\n",
    "repart_2_df = rawDF.repartition(2).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "repart_4_df = rawDF.repartition(4).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "repart_8_df = rawDF.repartition(8).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "repart_16_df = rawDF.repartition(16).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "repart_32_df = rawDF.repartition(32).persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "\n",
    "repart_2_df.registerTempTable(\"repart_2\")\n",
    "repart_4_df.registerTempTable(\"repart_4\")\n",
    "repart_8_df.registerTempTable(\"repart_8\")\n",
    "repart_16_df.registerTempTable(\"repart_16\")\n",
    "repart_32_df.registerTempTable(\"repart_32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform a simple job (count the number of elements in each dataframe) and compare how long the operation takes for various number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run through each DataFrame and count the number of elements in each. This will \n",
    "# trigger the DataFrames to persist into Memory and Disk\n",
    "df_arr = [(repart_2_df, 2), \n",
    "          (repart_4_df, 4), \n",
    "          (repart_8_df, 8), \n",
    "          (repart_16_df, 16), \n",
    "          (repart_32_df, 32)]\n",
    "for df in df_arr:\n",
    "    start_time = time()\n",
    "    df[0].count()\n",
    "    end_time = time()\n",
    "    print \"{} partitions took {} seconds to repartition and count\".format(df[1], end_time - start_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the runtime indicate about the scheduling and distribution of tasks to worker nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelism - example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function sorts an array of UTC time and calculates the delta time in days between each consecutive UTC time\n",
    "# Returns a Row object containing the subreddit, the median time delta representing the median time in days \n",
    "# between comments for any subreddit, and the total number of comments in each subreddit\n",
    "def calc_median(row):\n",
    "    subreddit = row[0]\n",
    "    val_arr = row[1]\n",
    "    sorted_arr = sorted(val_arr)\n",
    "    dt = []\n",
    "    if len(sorted_arr)>0:\n",
    "        for i in range(len(sorted_arr)-1):\n",
    "            dt.append(int(sorted_arr[i+1])-int(sorted_arr[i]))\n",
    "    else:\n",
    "        dt = [0]\n",
    "        \n",
    "    return Row(subreddit=subreddit, median_time_days=float(median(dt)/60/60/24), num_comments=len(sorted_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code is very inefficient. Can you figure out why (before attempting to run it)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop through each DataFrame and run the same job to calculate the median time between comments for each\n",
    "# subreddit\n",
    "for df in df_arr:\n",
    "    start_time = time()\n",
    "    curr_df = df[0]\n",
    "    num_partitions = df[1]\n",
    "    \n",
    "    subreddit_comment_times = curr_df.rdd.map(lambda r: (r.subreddit, r.created_utc))\n",
    "    median_time_between_posts_df = subreddit_comment_times.groupByKey()\\\n",
    "                                                          .map(calc_median)\\\n",
    "                                                          .toDF()\n",
    "    total_cnt = median_time_between_posts_df.count()\n",
    "    end_time = time()\n",
    "    print \"{} partitions took {} seconds to process, {} final records\".format(df[1], end_time - start_time, total_cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A scatter plot where each element is a subreddit. We can see that subreddits with more than 40 comments tend to\n",
    "# also have more frequent comment activity throughout the year\n",
    "median_time_between_posts_pd = median_time_between_posts_df.toPandas()\n",
    "median_time_between_posts_pd.plot(kind='scatter', x='num_comments', y='median_time_days', xlim=(-5, 100), ylim=(-5, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: What is the optimal number of partitions for calculating the author who has written the longest comment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
